## Neural Networks: Optimizers

### Notes:

- <b>Optimizer</b>: this is what will do the gradient descent and the backpropagation for us (this is what implements the backpropagation algorithm)
  - Types of algorihms:
    - Gradient Descent
    - Stochastic Gradient Descent
    - Mini-Bath Gradient Descent
    - Momentum
    - Nesterov Accelerated Gradient
- [More information](https://medium.com/game-of-bits/understanding-optimizers-for-training-deep-learning-models-694c071b5b70)
- Create the NN (see [main.py](main.py))

### Questions:

What is an optimizer function?

- A function that implements the gradient descent and backpropagation algorithms for you.
